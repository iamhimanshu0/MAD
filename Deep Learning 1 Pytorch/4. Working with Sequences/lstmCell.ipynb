{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "\n",
    "# https://github.com/georgeyiasemis/Recurrent-Neural-Networks-from-scratch-using-PyTorch/blob/main/rnnmodels.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM\n",
    "\n",
    "LSTM stands for Long Short-Term Memory, which is a type of recurrent neural network (RNN) architecture. LSTMs were designed to solve the problem of the vanishing gradient, which occurs when the gradients used to update the weights of a neural network become too small, leading to slow or no learning.\n",
    "\n",
    "LSTMs have a memory cell that can store information for long periods of time and three gates that control the flow of information: the input gate, the forget gate, and the output gate.\n",
    "\n",
    "The input gate determines how much of the new input should be stored in the memory cell, the forget gate controls how much of the old memory should be forgotten, and the output gate determines how much of the memory should be used to compute the output.\n",
    "\n",
    "The architecture of an LSTM allows it to selectively remember or forget information based on the current input and the past memory content. This makes LSTMs particularly well-suited for tasks that require modeling long-term dependencies, such as speech recognition, language translation, and stock price prediction.\n",
    "\n",
    "Overall, LSTMs are a powerful tool for modeling sequential data, and have become a popular choice in many applications that require time-series analysis or natural language processing.\n",
    "\n",
    "The LSTM architecture consists of several mathematical formulas, which are used to update the memory cell and control the flow of information through the gates. Here are the main formulas used in an LSTM:\n",
    "\n",
    "![lstm](https://dezyre.gumlet.io/images/blog/lstm-model/Long_Short_Term_Memory_(LSTM)_Models.png?w=400&dpr=2.6)\n",
    "\n",
    "1. Input gate formula:   \n",
    "i_t = sigmoid(W_i[x_t, h_{t-1}] + b_i)  \n",
    "The input gate is responsible for controlling how much of the current input x_t should be stored in the memory cell. It takes as input the concatenation of the current input x_t and the previous hidden state h_{t-1}, and produces a value between 0 and 1 using the sigmoid activation function.\n",
    "\n",
    "2. Forget gate formula:   \n",
    "f_t = sigmoid(W_f[x_t, h_{t-1}] + b_f)  \n",
    "The forget gate controls how much of the previous memory cell content should be retained for the current timestep. It takes as input the concatenation of the current input x_t and the previous hidden state h_{t-1}, and produces a value between 0 and 1 using the sigmoid activation function.\n",
    "\n",
    "3. Update cell state formula:   \n",
    "\\tilde{C}_t = tanh(W_C[x_t, h_{t-1}] + b_C)  \n",
    "This formula computes a \"candidate\" memory value that will be added to the memory cell state. It takes as input the concatenation of the current input x_t and the previous hidden state h_{t-1}, and produces a value between -1 and 1 using the hyperbolic tangent activation function.\n",
    "\n",
    "4. Update memory cell formula:   \n",
    "C_t = f_t * C_{t-1} + i_t * \\tilde{C}_t  \n",
    "This formula updates the memory cell state for the current timestep. It combines the previous memory cell state C_{t-1} with the candidate value \\tilde{C}_t, weighted by the forget gate f_t and the input gate i_t.\n",
    "\n",
    "5. Output gate formula:   \n",
    "o_t = sigmoid(W_o[x_t, h_{t-1}] + b_o) \n",
    "The output gate controls how much of the current memory cell state should be used to compute the output. It takes as input the concatenation of the current input x_t and the previous hidden state h_{t-1}, and produces a value between 0 and 1 using the sigmoid activation function.\n",
    "\n",
    "6. Hidden state formula:   \n",
    "h_t = o_t * tanh(C_t)  \n",
    "Finally, the hidden state for the current timestep is computed as a combination of the current memory cell state C_t and the output gate o_t, using the hyperbolic tangent activation function.\n",
    "\n",
    "These formulas work together to allow an LSTM to selectively retain or forget information over long time intervals, making it well-suited for tasks that require modeling long-term dependencies in sequential data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The LSTM cell has an input size (input_size) and a hidden size (hidden_size). bias is an optional argument that is set to True by default and adds a bias term to the linear transformations.\n",
    "\n",
    "The nn.Linear() function is used to define two linear transformations: self.xh which takes the input and applies a linear transformation to it, and self.hh which takes the previous hidden state and applies a linear transformation to it. The 4 in the hidden_size * 4 argument is because there are four gates in an LSTM cell: input gate, forget gate, cell gate, and output gate.\n",
    "\n",
    "The reset_parameters() function sets the parameters of the linear transformations to a uniform distribution between -std and std. The std is calculated as 1.0 / np.sqrt(self.hidden_size), which is a common initialization method for neural network weights.\n",
    "...\n",
    "Forward function\n",
    "...\n",
    "The function takes as input the input tensor of shape (batch_size, input_size) and an optional hidden state tensor hx of shape (batch_size, hidden_size).\n",
    "\n",
    "If hx is not provided, the function initializes it to a tensor of zeros with the same shape as the input tensor.\n",
    "The function then splits the hidden state tensor hx into two parts: hx and cx, which represent the hidden state and the cell state of the LSTM, respectively.\n",
    "\n",
    "The function computes the gates (i_t, f_t, g_t, o_t) using the input tensor input and the hidden state tensor hx. These gates represent the input gate, forget gate, cell gate, and output gate of the LSTM, respectively.\n",
    "\n",
    "The function applies the sigmoid activation function to the input gate and forget gate, and the hyperbolic tangent activation function to the cell gate and output gate.\n",
    "\n",
    "The function computes the new cell state cy by combining the old cell state cx with the input gate i_t and the cell gate g_t, and forgetting some of the old cell state using the forget gate f_t.\n",
    "\n",
    "The function computes the new hidden state hy by applying the output gate o_t to the cell state cy and applying the hyperbolic tangent activation function to the result.\n",
    "Finally, the function returns a tuple containing the new hidden state hy and the new cell state cy.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "class LSTMCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, bias=True):\n",
    "        super(LSTMCell, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.bias = bias \n",
    "\n",
    "        self.xh = nn.Linear(input_size, hidden_size*4, bias=bias)\n",
    "        self.hh = nn.Linear(hidden_size, hidden_size*4, bias=bias)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        std = 1.0/np.sqrt(self.hidden_size)\n",
    "        for w in self.parameters():\n",
    "            w.data.uniform_(-std, std)\n",
    "\n",
    "    def forward(self, input, hx=None):\n",
    "        # Inputs:\n",
    "        #       input: of shape (batch_size, input_size)\n",
    "        #       hx: of shape (batch_size, hidden_size)\n",
    "        # Outputs:\n",
    "        #       hy: of shape (batch_size, hidden_size)\n",
    "        #       cy: of shape (batch_size, hidden_size)\n",
    "\n",
    "        if hx is None:\n",
    "            hx = Variable(input.new_zeros(input.size(0), self.hidden_size))\n",
    "            hx = (hx, hx)\n",
    "\n",
    "        hx , cx = hx \n",
    "\n",
    "        gates = self.xh(input) + self.hh(hx)\n",
    "\n",
    "        # Get gates (i_t, f_t, g_t, o_t)\n",
    "        input_gate, forget_gate, cell_gate, output_gate = gates.chunk(4, 1)\n",
    "\n",
    "        i_t = torch.sigmoid(input_gate)\n",
    "        f_t = torch.sigmoid(forget_gate)\n",
    "        g_t = torch.tanh(cell_gate)\n",
    "        o_t = torch.sigmoid(output_gate)\n",
    "\n",
    "        cy = cx * f_t + i_t * g_t\n",
    "\n",
    "        hy = o_t * torch.tanh(cy)\n",
    "\n",
    "        return (hy, cy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "- In the constructor, the module takes as input the input size, hidden size, number of layers, bias flag, and output size of the LSTM.\n",
    "\n",
    "- The constructor initializes a list of LSTM cells (one for each layer) using the nn.ModuleList() class.\n",
    "- The first LSTM cell in the list takes the input size and hidden size as input and uses the bias flag to decide whether to include bias terms in the cell.\n",
    "\n",
    "- The remaining LSTM cells in the list take the hidden size as input and have the same bias flag as the first cell.\n",
    "\n",
    "- The forward() method of the module takes as input the input tensor of shape (batch_size, sequence_length, input_size) and an optional initial hidden state tensor hx of shape (num_layers, batch_size, hidden_size).\n",
    "\n",
    "- If hx is not provided, the method initializes it to a tensor of zeros with the appropriate shape.\n",
    "\n",
    "- The method then iterates over the sequence length of the input tensor, and for each time step, it iterates over the layers of the LSTM.\n",
    "\n",
    "- For each layer, the method computes the hidden state and cell state using the input tensor and the hidden state and cell state of the previous layer (or the initial hidden state if this is the first layer).\n",
    "\n",
    "- The method stores the output of the final time step in a list called outs and uses it to compute the final output of the module using a linear layer with output size output_size.\n",
    "\n",
    "- Finally, the method returns the final output tensor of shape (batch_size, output_size).\n",
    "\n",
    "Overall, this code defines a stacked LSTM module that takes a sequence of inputs and produces a sequence of outputs using multiple layers of LSTM cells.\n",
    "\"\"\"\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, bias, output_size):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.bias = bias\n",
    "        self.output_size = output_size\n",
    "\n",
    "        self.rnn_cell_list = nn.ModuleList()\n",
    "\n",
    "        self.rnn_cell_list.append(\n",
    "            LSTMCell(self.input_size, \n",
    "                     self.hidden_size, self.bias))\n",
    "\n",
    "        for l in range(1, self.num_layers):\n",
    "            self.rnn_cell_list.append(\n",
    "                LSTMCell(self.hidden_size, self.hidden_size, self.bias)\n",
    "            )\n",
    "        \n",
    "        self.fc = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hx=None):\n",
    "        # Input of shape (batch_size, seqence length , input_size)\n",
    "        #\n",
    "        # Output of shape (batch_size, output_size)\n",
    "\n",
    "        if hx is None:\n",
    "            if torch.cuda.is_available():\n",
    "                h0 = Variable(torch.zeros(self.num_layers,\n",
    "                              input.size(0), self.hidden_size).cuda())\n",
    "            else:\n",
    "                h0 = Variable(torch.zeros(self.num_layers,\n",
    "                              input.size(0), self.hidden_size))\n",
    "        else:\n",
    "            h0 = hx\n",
    "\n",
    "        outs = []\n",
    "        hidden = list()\n",
    "\n",
    "        for layer in range(self.num_layers):\n",
    "            hidden.append((h0[layer, :, :], h0[layer, :, :]))\n",
    "        \n",
    "        for t in range(input.size(1)):\n",
    "            for layer in range(self.num_layers):\n",
    "                if layer == 0:\n",
    "                    hidden_l = self.rnn_cell_list[layer](\n",
    "                        input[:, t, :],\n",
    "                        (hidden[layer][0], hidden[layer][1])\n",
    "                    )\n",
    "                else:\n",
    "                    hidden_l = self.rnn_cell_list[layer](\n",
    "                        hidden[layer-1][0],\n",
    "                        (hidden[layer][0], hidden[layer][1])\n",
    "                    )\n",
    "                \n",
    "                hidden[layer] = hidden_l\n",
    "            outs.append(hidden_l[0])\n",
    "            \n",
    "        out = outs[-1].squeeze()\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 100\n",
    "hidden_size = 5\n",
    "num_layers = 5\n",
    "bias = True\n",
    "output_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM(\n",
       "  (rnn_cell_list): ModuleList(\n",
       "    (0): LSTMCell(\n",
       "      (xh): Linear(in_features=100, out_features=20, bias=True)\n",
       "      (hh): Linear(in_features=5, out_features=20, bias=True)\n",
       "    )\n",
       "    (1): LSTMCell(\n",
       "      (xh): Linear(in_features=5, out_features=20, bias=True)\n",
       "      (hh): Linear(in_features=5, out_features=20, bias=True)\n",
       "    )\n",
       "    (2): LSTMCell(\n",
       "      (xh): Linear(in_features=5, out_features=20, bias=True)\n",
       "      (hh): Linear(in_features=5, out_features=20, bias=True)\n",
       "    )\n",
       "    (3): LSTMCell(\n",
       "      (xh): Linear(in_features=5, out_features=20, bias=True)\n",
       "      (hh): Linear(in_features=5, out_features=20, bias=True)\n",
       "    )\n",
       "    (4): LSTMCell(\n",
       "      (xh): Linear(in_features=5, out_features=20, bias=True)\n",
       "      (hh): Linear(in_features=5, out_features=20, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=5, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LSTM(input_size, hidden_size, num_layers, bias, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2])\n",
      "tensor([[ 0.5334, -0.0565],\n",
      "        [ 0.5318, -0.0566]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Define input tensor of shape (batch_size, sequence_length, input_size)\n",
    "input_tensor = torch.rand((2, 5, 3))\n",
    "\n",
    "# Define LSTM module with input size=3, hidden size=4, 2 layers, bias=True, output size=2\n",
    "lstm = LSTM(input_size=3, hidden_size=4,\n",
    "            num_layers=2, bias=True, output_size=2)\n",
    "\n",
    "# Forward pass through the LSTM module\n",
    "output_tensor = lstm(input_tensor)\n",
    "\n",
    "# Print the shape of the output tensor\n",
    "print(output_tensor.shape)\n",
    "print(output_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
