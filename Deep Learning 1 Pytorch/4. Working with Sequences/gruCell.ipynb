{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU\n",
    "\n",
    "The GRU has a similar structure to the LSTM, but with fewer parameters. It consists of two gates: a reset gate and an update gate. The reset gate determines how much of the previous hidden state is retained, and the update gate determines how much of the new input is added to the hidden state.  \n",
    "\n",
    "![gru](https://www.researchgate.net/publication/343002752/figure/fig1/AS:914127664979968@1594956427113/The-Architecture-of-basic-Gated-Recurrent-Unit-GRU.ppm)\n",
    "\n",
    "The GRU update equations are as follows:\n",
    "\n",
    "Reset gate: $r_t = \\sigma(W_r x_t + U_r h_{t-1} + b_r)$  \n",
    "Update gate: $z_t = \\sigma(W_z x_t + U_z h_{t-1} + b_z)$  \n",
    "Candidate activation: $\\tilde{h_t} = \\tanh(W x_t + r_t \\odot U h_{t-1} + b)$  \n",
    "Hidden state: $h_t = (1 - z_t) \\odot h_{t-1} + z_t \\odot \\tilde{h_t}$  \n",
    "\n",
    "where:\n",
    "\n",
    "$x_t$ is the input at time step $t$.    \n",
    "$h_{t-1}$ is the hidden state at the previous time step.  \n",
    "$r_t$ is the reset gate at time step $t$.  \n",
    "$z_t$ is the update gate at time step $t$.  \n",
    "$\\tilde{h_t}$ is the candidate activation at time step $t$.   \n",
    "$h_t$ is the new hidden state at time step $t$.  \n",
    "$W_r$, $U_r$, $b_r$, $W_z$, $U_z$, $b_z$, $W$, $U$, and $b$ are learnable parameters.  \n",
    "Like the LSTM, the GRU can be stacked to create deeper architectures, and it can be bidirectional to capture information from both past and future time steps.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The model is defined in the GRU class, which inherits from the nn.Module class in PyTorch. The constructor method (__init__) initializes the parameters of the model, including the input size, hidden size, number of layers, bias, and output size. It also creates a list of GRU cells (self.rnn_cell_list), which will be used to compute the hidden state at each time step of the input sequence.\n",
    "\n",
    "The forward method defines the forward pass of the model. It takes as input a tensor input of shape (batch_size, seqence length, input_size), and an optional initial hidden state hx. If hx is not provided, it is initialized to a tensor of zeros.\n",
    "\n",
    "The method then iterates over the time steps of the input sequence, and computes the hidden state at each time step using the GRU cells in self.rnn_cell_list. The hidden state of each layer is stored in the hidden list, which is initialized to hx if provided, or to zeros otherwise.\n",
    "\n",
    "The `GRU` class inherits from `nn.Module` and defines a GRU architecture for sequence classification. It takes in the following arguments:\n",
    "\n",
    "- `input_size`: The number of expected features in the input.\n",
    "- `hidden_size`: The number of features in the hidden state `h`.\n",
    "- `num_layers`: Number of recurrent layers. E.g., setting `num_layers=2` would mean stacking two GRUs together to form a `stacked GRU`, with the second GRU taking in outputs of the first GRU and producing the final results.\n",
    "- `bias`: If `False`, then the layer does not use bias weights `b_ih` and `b_hh`. Default: `True`.\n",
    "- `output_size`: The size of the output layer, which is the number of classes in the classification problem.\n",
    "\n",
    "The `__init__` method initializes the architecture by creating a list of `GRUCell`s based on the number of layers specified. It also creates a linear layer `fc` to convert the final hidden state into the desired output size. \n",
    "\n",
    "The `forward` method takes in the input tensor `input` of shape `(batch_size, sequence_length, input_size)` and an optional initial hidden state `hx`. The method first initializes the initial hidden state `h0` to zeros if `hx` is not provided. \n",
    "\n",
    "The method then loops through the time steps of the input tensor, passing the input and hidden states through each layer of the GRU. The hidden states are stored in the `hidden` list, with each element of the list corresponding to the hidden state at each layer. The output at each time step is stored in the `outs` list.\n",
    "\n",
    "After the loop, the method takes the last output from the `outs` list, applies a linear layer `fc` to it, and returns the final output tensor of shape `(batch_size, output_size)`.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "class GRUCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, bias=True):\n",
    "        super(GRUCell, self).__init__()\n",
    "        self.input_size = input\n",
    "        self.hidden_size = hidden_size\n",
    "        self.bias = bias \n",
    "\n",
    "        self.x2h = nn.Linear(input_size, 3*hidden_size, bias=bias)\n",
    "        self.h2h = nn.Linear(hidden_size, 3*hidden_size, bias=bias)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        std = 1.0 / np.sqrt(self.hidden_size)\n",
    "        for w in self.parameters():\n",
    "            w.data.uniform_(-std, std)\n",
    "\n",
    "    def forward(self, input, hx=None):\n",
    "        # Inputs:\n",
    "        #       input: of shape (batch_size, input_size)\n",
    "        #       hx: of shape (batch_size, hidden_size)\n",
    "        # Output:\n",
    "        #       hy: of shape (batch_size, hidden_size)\n",
    "\n",
    "        if hx is None:\n",
    "            hx = Variable(input.new_zeros(input.size(0), self.hidden_size))\n",
    "\n",
    "        x_t = self.x2h(input)\n",
    "        h_t = self.h2h(hx)\n",
    "\n",
    "        x_reset, x_upd, x_new = x_t.chunk(3,1)\n",
    "        h_reset, h_upd, h_new = h_t.chunk(3,1)\n",
    "\n",
    "        reset_gate = torch.sigmoid(x_reset + h_reset)\n",
    "        update_gate = torch.sigmoid(x_upd + h_upd)\n",
    "        new_gate = torch.tanh(x_new + (reset_gate * h_new))\n",
    "\n",
    "        hy = update_gate * hx + (1 - update_gate) * new_gate\n",
    "\n",
    "        return hy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, bias, output_size):\n",
    "        super(GRU, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.bias = bias \n",
    "        self.output_size = output_size\n",
    "\n",
    "        self.rnn_cell_list = nn.ModuleList()\n",
    "\n",
    "        self.rnn_cell_list.append(\n",
    "            GRUCell(\n",
    "                self.input_size,\n",
    "                self.hidden_size,\n",
    "                self.bias\n",
    "            )\n",
    "        )\n",
    "\n",
    "        for l in range(1, self.num_layers):\n",
    "            self.rnn_cell_list.append(GRUCell(self.hidden_size,\n",
    "                                              self.hidden_size,\n",
    "                                              self.bias))\n",
    "        self.fc = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hx=None):\n",
    "\n",
    "        # Input of shape (batch_size, seqence length, input_size)\n",
    "        #\n",
    "        # Output of shape (batch_size, output_size)\n",
    "\n",
    "        if hx is None:\n",
    "            if torch.cuda.is_available():\n",
    "                h0 = Variable(torch.zeros(self.num_layers,\n",
    "                              input.size(0), self.hidden_size).cuda())\n",
    "            else:\n",
    "                h0 = Variable(torch.zeros(self.num_layers,\n",
    "                              input.size(0), self.hidden_size))\n",
    "\n",
    "        else:\n",
    "            h0 = hx\n",
    "\n",
    "        outs = []\n",
    "\n",
    "        hidden = list()\n",
    "        for layer in range(self.num_layers):\n",
    "            hidden.append(h0[layer, :, :])\n",
    "\n",
    "        for t in range(input.size(1)):\n",
    "\n",
    "            for layer in range(self.num_layers):\n",
    "\n",
    "                if layer == 0:\n",
    "                    hidden_l = self.rnn_cell_list[layer](\n",
    "                        input[:, t, :], hidden[layer])\n",
    "                else:\n",
    "                    hidden_l = self.rnn_cell_list[layer](\n",
    "                        hidden[layer - 1], hidden[layer])\n",
    "                hidden[layer] = hidden_l\n",
    "\n",
    "                hidden[layer] = hidden_l\n",
    "\n",
    "            outs.append(hidden_l)\n",
    "\n",
    "        # Take only last time step. Modify for seq to seq\n",
    "        out = outs[-1].squeeze()\n",
    "\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2])\n",
      "tensor([[ 0.0959, -0.3729],\n",
      "        [ 0.0868, -0.3723]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Define input tensor of shape (batch_size, sequence_length, input_size)\n",
    "input_tensor = torch.rand((2, 5, 3))\n",
    "\n",
    "# Define LSTM module with input size=3, hidden size=4, 2 layers, bias=True, output size=2\n",
    "gru = GRU(input_size=3, hidden_size=4,\n",
    "            num_layers=2, bias=True, output_size=2)\n",
    "\n",
    "# Forward pass through the LSTM module\n",
    "output_tensor = gru(input_tensor)\n",
    "\n",
    "# Print the shape of the output tensor\n",
    "print(output_tensor.shape)\n",
    "print(output_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
