{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN\n",
    "\n",
    "Recurrent Neural Networks (RNNs) are a type of neural network that are used to model sequential data such as time series, audio, natural language processing, and many others. The fundamental difference between RNNs and other neural networks is that they maintain a state, or \"memory\", of previous inputs that they've seen. This makes them particularly well-suited to tasks that require processing sequential information.\n",
    "\n",
    "An RNN consists of a series of nodes, or \"cells\", that pass information from one to the next. Each cell takes an input and a hidden state from the previous cell as input, and produces an output and a new hidden state as output. The output is typically fed into a classifier or another neural network to produce a final prediction.\n",
    "\n",
    "![rnn](https://pluralsight2.imgix.net/guides/f6c9b982-4be1-48d5-9f44-bcf7f772eccd_4.JPG)\n",
    "\n",
    "The simplest RNN architecture is the Elman network, which consists of a single hidden layer. The input at each time step is fed into the input layer, and then processed by the hidden layer, which maintains a state that is passed on to the next time step. The output is produced by a linear or softmax layer that takes the hidden state as input.\n",
    "\n",
    "A more advanced type of RNN architecture is the Long Short-Term Memory (LSTM) network, which was designed to address the problem of vanishing gradients in Elman networks. LSTMs use a more complex cell structure that consists of several \"gates\" that control the flow of information through the cell. This allows LSTMs to better maintain long-term dependencies in sequential data.\n",
    "\n",
    "here is the mathematical formula for a basic RNN with one hidden layer:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "h_t &= \\sigma(W_{xh} x_t + W_{hh} h_{t-1} + b_h) \\\\\n",
    "y_t &= \\mathrm{softmax}(W_{hy} h_t + b_y) \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $h_t$ is the hidden state at time $t$\n",
    "- $x_t$ is the input at time $t$\n",
    "- $y_t$ is the output at time $t$\n",
    "- $W_{xh}$ is the weight matrix for input-to-hidden connections\n",
    "- $W_{hh}$ is the weight matrix for hidden-to-hidden connections\n",
    "- $W_{hy}$ is the weight matrix for hidden-to-output connections\n",
    "- $b_h$ is the bias term for the hidden layer\n",
    "- $b_y$ is the bias term for the output layer\n",
    "- $\\sigma$ is the activation function, typically sigmoid or tanh\n",
    "- $\\mathrm{softmax}$ is the output activation function, used for classification problems to produce a probability distribution over classes.\n",
    "\n",
    "Note that this formula assumes a simple RNN with only one hidden layer. More complex architectures, such as stacked or bidirectional RNNs, may have additional layers or connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, bias=True,nonlinearity='tanh'):\n",
    "        super(RNNCell, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.bias = bias\n",
    "        self.nonlinearity = nonlinearity\n",
    "\n",
    "        if self.nonlinearity not in ['relu', 'tanh']:\n",
    "            raise ValueError(\"Invalid nonlinearity selected for RNN.\")\n",
    "\n",
    "        self.x2h = nn.Linear(input_size, hidden_size, bias=bias)\n",
    "        self.h2h = nn.Linear(hidden_size, hidden_size, bias)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        std = 1.0 / np.sqrt(self.hidden_size)\n",
    "        for w in self.parameters():\n",
    "            w.data.uniform_(-std, std)\n",
    "\n",
    "    \n",
    "    def forward(self, input, hx=None):\n",
    "        # Inputs:\n",
    "        #       input: of shape (batch_size, input_size)\n",
    "        #       hx: of shape (batch_size, hidden_size)\n",
    "        # Output:\n",
    "        #       hy: of shape (batch_size, hidden_size)\n",
    "\n",
    "        if hx is None:\n",
    "            hx = Variable(input.new_zeros(input.size(0), self.hidden_size))\n",
    "\n",
    "        hy = (self.x2h(input) + self.h2h(hx))\n",
    "\n",
    "        if self.nonlinearity == 'tanh':\n",
    "            hy = torch.tanh(hy)\n",
    "        else:\n",
    "            hy = torch.relu(hy)\n",
    "\n",
    "        return hy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, bias, output_size, activation='tanh'):\n",
    "        super(RNN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.bias = bias\n",
    "        self.output_size = output_size\n",
    "\n",
    "        self.rnn_cell_list = nn.ModuleList()\n",
    "\n",
    "        if activation == \"tanh\":\n",
    "            self.rnn_cell_list.append(\n",
    "                RNNCell(\n",
    "                    self.input_size,\n",
    "                    self.hidden_size, \n",
    "                    self.bias,\n",
    "                    \"tanh\"\n",
    "                )\n",
    "            )\n",
    "\n",
    "            for l in range(1, self.num_layers):\n",
    "                self.rnn_cell_list.append(\n",
    "                    RNNCell(\n",
    "                        self.hidden_size,\n",
    "                        self.hidden_size,\n",
    "                        self.bias,\n",
    "                        \"tanh\"\n",
    "                    )\n",
    "                )\n",
    "        elif activation == \"relu\":\n",
    "            self.rnn_cell_list.append(\n",
    "                RNNCell(\n",
    "                    self.input_size,\n",
    "                    self.hidden_size, \n",
    "                    self.bias,\n",
    "                    \"relu\"\n",
    "                )\n",
    "            )\n",
    "\n",
    "            for l in range(1, self.num_layers):\n",
    "                self.rnn_cell_list.append(\n",
    "                    RNNCell(\n",
    "                        self.hidden_size,\n",
    "                        self.hidden_size,\n",
    "                        self.bias,\n",
    "                        \"relu\"\n",
    "                    )\n",
    "                )\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(\"Invalid Activation\")\n",
    "        \n",
    "        self.fc = nn.Linear(\n",
    "            self.hidden_size,\n",
    "            self.output_size\n",
    "        )\n",
    "\n",
    "    def forward(self, input, hx=None):\n",
    "        # Input of shape (batch_size, seqence length, input_size)\n",
    "        #\n",
    "        # Output of shape (batch_size, output_size)\n",
    "\n",
    "        if hx is None:\n",
    "            if torch.cuda.is_available():\n",
    "                h0 = Variable(torch.zeros(self.num_layers, input.size(0), self.hidden_size).cuda())\n",
    "            else:\n",
    "                h0 = Variable(torch.zeros(self.num_layers, input.size(0), self.hidden_size))\n",
    "        else:\n",
    "            h0 = hx \n",
    "        \n",
    "        outs = []\n",
    "\n",
    "        hidden = list()\n",
    "        for layer in range(self.num_layers):\n",
    "            hidden.append(h0[layer, :, :])\n",
    "\n",
    "        for t in range(input.size(1)):\n",
    "            for layer in range(self.num_layers):\n",
    "                if layer == 0:\n",
    "                    hidden_l = self.rnn_cell_list[layer](input[:,t,:], hidden[layer])\n",
    "                else:\n",
    "                    hidden_l = self.rnn_cell_list[layer](hidden[layer-1], hidden[layer])\n",
    "                hidden[layer] = hidden_l\n",
    "\n",
    "            outs.append(hidden_l)\n",
    "        \n",
    "        # Take only last time step. modify for seq to seq\n",
    "        out = outs[-1].squeeze()\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2])\n",
      "tensor([[-0.4781, -0.2210],\n",
      "        [-0.4721, -0.1940]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Define input tensor of shape (batch_size, sequence_length, input_size)\n",
    "input_tensor = torch.rand((2, 5, 3))\n",
    "\n",
    "# Define LSTM module with input size=3, hidden size=4, 2 layers, bias=True, output size=2\n",
    "rnn = RNN(input_size=3, hidden_size=4,\n",
    "            num_layers=2, bias=True, output_size=2)\n",
    "\n",
    "# Forward pass through the LSTM module\n",
    "output_tensor = rnn(input_tensor)\n",
    "\n",
    "# Print the shape of the output tensor\n",
    "print(output_tensor.shape)\n",
    "print(output_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_tensor = torch.rand((2, 5, 3))\n",
    "# print(input_tensor)\n",
    "# Variable(input_tensor.new_zeros(input_tensor.size(0), 3))\n",
    "\n",
    "\"\"\"\n",
    "nn.Linear() is a PyTorch module that applies a linear transformation to the input tensor, i.e., it performs a matrix multiplication followed by a bias addition.\n",
    "\n",
    "The output tensor has the shape (batch_size, out_features) where out_features is the number of output features specified during module initialization. The input tensor should have the shape (batch_size, in_features) where in_features is the number of input features.\n",
    "\"\"\"\n",
    "input_size = 100\n",
    "hidden_size = 5\n",
    "bias = True\n",
    "# nn.Linear(input_size, hidden_size, bias=bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
